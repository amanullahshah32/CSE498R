{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanullahshah32/CSE498R/blob/main/SentimentAnlysisWithXAI%2BBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Required Libraries\n"
      ],
      "metadata": {
        "id": "LEcji_7cK0zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torch-optimizer imbalanced-learn scikit-learn matplotlib --quiet\n"
      ],
      "metadata": {
        "id": "m1E5mWsOH-KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Load and Preprocess The Data"
      ],
      "metadata": {
        "id": "v5EaO7JvLe8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your scraped data from the provided URL\n",
        "url = \"https://raw.githubusercontent.com/amanullahshah32/Review-Scraping/refs/heads/main/Dataset/cleaned_dataset.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Drop rows where 'review_description' or 'rating' are missing\n",
        "df.dropna(subset=['review_description', 'rating'], inplace=True)\n",
        "\n",
        "# Shuffle the sampled dataset\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Create a sentiment column based on rating (assuming rating scale is 1-5)\n",
        "df['sentiment'] = df['rating'].apply(lambda x: 0 if x <= 2 else (1 if x == 3 else 2))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['review_description'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to list\n",
        "train_labels = train_labels.tolist()\n",
        "val_labels = val_labels.tolist()\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "xfAOXKaWKRIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Handle Class Imbalance"
      ],
      "metadata": {
        "id": "eWEr9nyXL4Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Since train_texts is a pandas Series, we need to reshape it to a DataFrame\n",
        "train_texts_df = pd.DataFrame(train_texts)\n",
        "\n",
        "# Apply oversampling to balance the classes\n",
        "train_texts_resampled, train_labels_resampled = ros.fit_resample(train_texts_df, train_labels)\n",
        "\n",
        "# Convert the DataFrame of resampled texts back to a list\n",
        "train_texts_resampled = train_texts_resampled.squeeze().tolist()  # .squeeze() ensures a flat list\n"
      ],
      "metadata": {
        "id": "2e0O7bxKKRFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Tokenization with BERT"
      ],
      "metadata": {
        "id": "c0kB6QxdML4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text data\n",
        "train_encodings = tokenizer(train_texts_resampled, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "kkkz2nXkKRCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Create a Dataset Class for PyTorch"
      ],
      "metadata": {
        "id": "Xdm373srMPwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create the PyTorch datasets\n",
        "train_dataset = ReviewDataset(train_encodings, train_labels_resampled)\n",
        "val_dataset = ReviewDataset(val_encodings, val_labels)\n"
      ],
      "metadata": {
        "id": "x9BfMmJ8KQ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Load Pre-trained BERT Model"
      ],
      "metadata": {
        "id": "gO_BMLK5MTv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Define the device (use GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification (3 classes)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "BZI3CowORevo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Set Up DataLoader, Optimizer, and Scheduler"
      ],
      "metadata": {
        "id": "TpNjEyjoMlD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Optimizer: AdamW with weight decay and a smaller learning rate\n",
        "learning_rate = 3e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "# Scheduler for learning rate decay\n",
        "epochs = 10\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF_emcv1KQ3x",
        "outputId": "b964ef0c-7dcb-4f22-f9b4-ad0193640cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Class Weights for Imbalance"
      ],
      "metadata": {
        "id": "BXqcvpzGMquy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Convert the class labels to a NumPy array\n",
        "classes = np.array([0, 1, 2])\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=train_labels_resampled)\n",
        "\n",
        "# Convert to a PyTorch tensor and move it to the appropriate device\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Use the weights in the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "XyXeDtn6KoHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Training Loop"
      ],
      "metadata": {
        "id": "z8ZDL5_IMyzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to track metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_durations = []\n",
        "\n",
        "# Loop for training and validation\n",
        "for epoch in range(10):  # Training for 10 epochs\n",
        "    start_time = time.time()  # Start time for the epoch\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    train_preds = []\n",
        "    train_labels_epoch = []  # Track labels for each epoch\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Collect predictions\n",
        "        train_preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
        "        train_labels_epoch.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    end_time = time.time()  # End time for the epoch\n",
        "    epoch_duration = end_time - start_time  # Time taken for the epoch\n",
        "    epoch_durations.append(epoch_duration)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_acc = accuracy_score(train_labels_epoch, train_preds)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            val_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f'Epoch {epoch+1} completed in {epoch_duration:.2f} seconds')\n",
        "    print(f'Training Accuracy: {train_acc:.4f}')\n",
        "    print(f'Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "    # Classification report\n",
        "    print(f'Classification Report (Validation):\\n {classification_report(val_labels, val_preds)}')\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, epochs+1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, epochs+1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35m0mC39KoEK",
        "outputId": "20aca50c-1e15-457d-aa40-b75d63e22e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed in 2661.51 seconds\n",
            "Training Accuracy: 0.8774\n",
            "Validation Accuracy: 0.8876\n",
            "Classification Report (Validation):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.75      0.74      1312\n",
            "           1       0.14      0.25      0.18       449\n",
            "           2       0.96      0.93      0.94     11293\n",
            "\n",
            "    accuracy                           0.89     13054\n",
            "   macro avg       0.61      0.64      0.62     13054\n",
            "weighted avg       0.91      0.89      0.90     13054\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed in 2659.65 seconds\n",
            "Training Accuracy: 0.9529\n",
            "Validation Accuracy: 0.8867\n",
            "Classification Report (Validation):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.77      0.73      1312\n",
            "           1       0.12      0.18      0.15       449\n",
            "           2       0.96      0.93      0.94     11293\n",
            "\n",
            "    accuracy                           0.89     13054\n",
            "   macro avg       0.59      0.62      0.61     13054\n",
            "weighted avg       0.90      0.89      0.89     13054\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 completed in 2658.48 seconds\n",
            "Training Accuracy: 0.9614\n",
            "Validation Accuracy: 0.8697\n",
            "Classification Report (Validation):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.73      0.71      1312\n",
            "           1       0.10      0.20      0.14       449\n",
            "           2       0.95      0.91      0.93     11293\n",
            "\n",
            "    accuracy                           0.87     13054\n",
            "   macro avg       0.58      0.61      0.59     13054\n",
            "weighted avg       0.90      0.87      0.88     13054\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed in 2658.67 seconds\n",
            "Training Accuracy: 0.9655\n",
            "Validation Accuracy: 0.8867\n",
            "Classification Report (Validation):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.68      0.71      1312\n",
            "           1       0.11      0.15      0.13       449\n",
            "           2       0.94      0.94      0.94     11293\n",
            "\n",
            "    accuracy                           0.89     13054\n",
            "   macro avg       0.60      0.59      0.59     13054\n",
            "weighted avg       0.89      0.89      0.89     13054\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Make New Predictions"
      ],
      "metadata": {
        "id": "xdWqTp_qM5Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on new data (Example: a list of review texts)\n",
        "new_reviews = [\n",
        "    \"The app is very helpful for tracking my health.\",\n",
        "    \"I had a bad experience, it kept crashing.\",\n",
        "    \"Great app, I would definitely recommend it to others!\"\n",
        "]\n",
        "\n",
        "# Tokenize the new reviews\n",
        "new_encodings = tokenizer(new_reviews, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Move the tensors to the appropriate device\n",
        "new_encodings = {key: val.to(device) for key, val in new_encodings.items()}\n",
        "\n",
        "# Perform the prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**new_encodings)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "# Print the predictions (0 = Negative, 1 = Neutral, 2 = Positive)\n",
        "for review, pred in zip(new_reviews, predictions):\n",
        "    sentiment = ['Negative', 'Neutral', 'Positive'][pred]\n",
        "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "id": "Mx_FZxzXKoBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. SHAP for Model Explanation"
      ],
      "metadata": {
        "id": "XnjhFBXiM-az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Create a SHAP explainer using the model and tokenizer\n",
        "explainer = shap.Explainer(model, tokenizer)\n",
        "\n",
        "# Choose a random sample from the validation set to explain\n",
        "sample_review = val_texts[0]  # You can change this index to any text in the validation set\n",
        "sample_encoding = tokenizer(sample_review, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Explanation using SHAP\n",
        "shap_values = explainer(sample_encoding)\n",
        "\n",
        "# Visualize the SHAP values for this prediction\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, sample_encoding['input_ids'])\n",
        "\n"
      ],
      "metadata": {
        "id": "bJniHtWLKvJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. LIME for Model Explanation"
      ],
      "metadata": {
        "id": "KjmFzGEKRA9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Initialize a LIME text explainer\n",
        "explainer = LimeTextExplainer(class_names=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "# Define a function for predicting class probabilities using the BERT model\n",
        "def predict_proba(texts):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        logits = model(**encodings).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "# Choose a sample review from the validation set\n",
        "sample_review = val_texts[0]\n",
        "\n",
        "# Explain the prediction for the sample review using LIME\n",
        "lime_explanation = explainer.explain_instance(sample_review, predict_proba, num_features=10)\n",
        "\n",
        "# Visualize the LIME explanation\n",
        "lime_explanation.show_in_notebook(text=True)\n"
      ],
      "metadata": {
        "id": "6l9pFVrIQ8z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6OBVjOEQ8wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}